"""
SafeTensors Demo for Unified LLM System
Demonstrates SafeTensors integration and benefits
"""

from unified_llm import UnifiedLLM, UserLLMManager, HelloWorldDataset
import os
import time


def safetensors_demo():
    """Demonstrate SafeTensors functionality"""
    print("ğŸ”’ SAFETENSORS DEMO")
    print("="*50)
    
    manager = UserLLMManager()
    dataset = HelloWorldDataset()
    
    print("\n1. Creating Neural model with SafeTensors...")
    
    # Create and train neural model
    neural_model = manager.create_user_model("safetensors_user", "neural", vocab_size=40)
    neural_model.train(dataset.texts)
    
    # Save using SafeTensors
    print("\nğŸ“¦ Saving model in SafeTensors format...")
    start_time = time.time()
    neural_model.save("safetensors_demo")
    save_time = time.time() - start_time
    
    print(f"â±ï¸  Save time: {save_time:.3f} seconds")
    
    print("\n2. Loading model from SafeTensors...")
    
    # Load using SafeTensors
    start_time = time.time()
    loaded_model = manager.load_user_model("safetensors_user", "safetensors_demo")
    load_time = time.time() - start_time
    
    print(f"â±ï¸  Load time: {load_time:.3f} seconds")
    
    print("\n3. Testing model functionality...")
    
    # Test generation
    test_prompts = ["hello", "hi", "good"]
    print("\nğŸ¯ Generated text:")
    for prompt in test_prompts:
        result = loaded_model.generate(prompt, max_length=15, temperature=0.8)
        print(f"  '{prompt}' â†’ '{result}'")
    
    print("\n4. File analysis...")
    
    # Check file sizes
    models_dir = f"models/safetensors_user"
    safetensors_file = os.path.join(models_dir, "safetensors_demo_neural.safetensors")
    vocab_file = os.path.join(models_dir, "safetensors_demo_vocab.json")
    
    if os.path.exists(safetensors_file):
        st_size = os.path.getsize(safetensors_file)
        vocab_size = os.path.getsize(vocab_file)
        print(f"ğŸ“„ SafeTensors file: {st_size/1024:.1f} KB")
        print(f"ğŸ“„ Vocabulary file: {vocab_size/1024:.1f} KB")
        print(f"ğŸ“„ Total size: {(st_size + vocab_size)/1024:.1f} KB")
    
    print("\nâœ… SafeTensors demo completed successfully!")


def compare_formats():
    """Compare SafeTensors vs legacy NPZ format"""
    print("\nğŸ†š FORMAT COMPARISON")
    print("="*40)
    
    manager = UserLLMManager()
    dataset = HelloWorldDataset()
    
    # Create two identical models
    print("Creating identical models for comparison...")
    
    # Model 1: SafeTensors (default for new models)
    model_st = manager.create_user_model("format_test_st", "neural", vocab_size=50)
    model_st.train(dataset.texts)
    
    print("\nğŸ“¦ Saving in SafeTensors format...")
    start_time = time.time()
    model_st.save("comparison_model")
    st_save_time = time.time() - start_time
    
    print("\nğŸ“¦ Loading from SafeTensors...")
    start_time = time.time()
    loaded_st = manager.load_user_model("format_test_st", "comparison_model")
    st_load_time = time.time() - start_time
    
    # Check file sizes
    st_dir = "models/format_test_st"
    st_model_file = os.path.join(st_dir, "comparison_model_neural.safetensors")
    st_vocab_file = os.path.join(st_dir, "comparison_model_vocab.json")
    
    st_total_size = 0
    if os.path.exists(st_model_file):
        st_total_size += os.path.getsize(st_model_file)
    if os.path.exists(st_vocab_file):
        st_total_size += os.path.getsize(st_vocab_file)
    
    print("\nğŸ“Š Performance Comparison:")
    print("-" * 40)
    print("SafeTensors Format:")
    print(f"  ğŸ’¾ Total size: {st_total_size/1024:.1f} KB")
    print(f"  ğŸ’¾ Save time: {st_save_time:.3f}s")
    print(f"  ğŸ’¾ Load time: {st_load_time:.3f}s")
    
    print("\nğŸ”’ SafeTensors Benefits:")
    print("  âœ… Memory safe - no arbitrary code execution")
    print("  âœ… Fast loading with memory mapping")
    print("  âœ… Cross-platform compatibility")
    print("  âœ… Lazy loading support")
    print("  âœ… Better error handling")
    print("  âœ… Metadata validation")


def migration_demo():
    """Demonstrate migration from legacy format"""
    print("\nğŸ”„ MIGRATION DEMO")
    print("="*30)
    
    manager = UserLLMManager()
    
    # Check for legacy models
    users = manager.list_users()
    legacy_found = False
    
    for user in users:
        models = manager.list_user_models(user)
        for model_info in models:
            if model_info.get('format') == 'legacy_npz':
                legacy_found = True
                print(f"ğŸ“¦ Found legacy model: {user}/{model_info['name']}")
    
    if legacy_found:
        print("\nğŸ”„ Converting legacy models to SafeTensors...")
        converted, failed = manager.batch_convert_legacy_models()
        
        if converted > 0:
            print(f"\nâœ… Successfully converted {converted} models to SafeTensors!")
        if failed > 0:
            print(f"âš ï¸  {failed} models failed to convert")
    else:
        print("â„¹ï¸  No legacy models found. All models are using modern formats!")
    
    print("\nğŸ“‹ Current model inventory:")
    for user in users:
        models = manager.list_user_models(user)
        if models:
            print(f"\nğŸ‘¤ {user}:")
            for model_info in models:
                format_icon = "ğŸ”’" if model_info.get('format') == 'safetensors' else "ğŸ“„"
                print(f"  {format_icon} {model_info['name']} ({model_info['type']}, {model_info.get('format', 'unknown')})")


def main():
    print("ğŸš€ SAFETENSORS UNIFIED LLM DEMO")
    print("="*60)
    
    print("\nChoose demo:")
    print("1. SafeTensors Basic Demo")
    print("2. Format Comparison")
    print("3. Legacy Migration Demo")
    print("4. All Demos")
    
    choice = input("\nChoice (1-4): ").strip()
    
    if choice == "1":
        safetensors_demo()
    elif choice == "2":
        compare_formats()
    elif choice == "3":
        migration_demo()
    else:
        safetensors_demo()
        compare_formats()
        migration_demo()
    
    print("\n" + "="*60)
    print("ğŸ‰ DEMO COMPLETE!")
    print("âœ… Your LLM system now uses SafeTensors for secure model storage")
    print("âœ… Faster loading, better security, cross-platform compatibility")
    print("="*60)


if __name__ == "__main__":
    main()
